# -*- coding: utf-8 -*-
"""Homework1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p55DnKlj83eKjoDF8sBsD5sOm0pNSgJ7

IMPORT LIBRARIES
"""

import numpy as np
import pandas as pd
import random
import os

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import *
from sklearn.naive_bayes import *
from sklearn.metrics import confusion_matrix, classification_report

print('Libraries imported.')

"""Setting up the environment"""

from google.colab import drive
drive.mount('/content/drive')

"""Importing the dataset through pandas"""

import json
filename = '/content/drive/My Drive/ML/noduplicatedataset.json'
db = pd.read_json(filename, lines=True)
print('File loaded: %d samples.\n\n' %(len(db.index)))

#print(db)
# Print a random item
print('%d %s %s' %(id,db.semantic[38],db.lista_asm[38]))

###
# From now on the elements of the json file can b accessed
# as they were attributes of the object db
# e.g. db.semantic[0] will return 'string'
# which is the semantic of the firt tuple of the json file
###

"""Compute vectorizer terms for all instances"""

# In this way we are able to manipulate the file as a vector of numbers

db.data = db.lista_asm
db.target = db.semantic

vectorizer = CountVectorizer() # multivariate

X_all = vectorizer.fit_transform(db.data)
y_all = db.target
print(vectorizer.get_feature_names())

print(X_all.shape)
print(y_all.shape)

"""Split data"""

X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, 
          test_size=0.333, random_state=117)

print("Size of training set: %d" %X_train.shape[0])
print("Size of test set: %d" %X_test.shape[0])

"""Fit classifier

**Choose model to evaluate:**
"""

from sklearn import tree
from sklearn import svm

classifier_name = 'S'

ClassifierMap = {
    'D':[tree.DecisionTreeClassifier],
    'S': [svm.LinearSVC, 'SVM']
    }

classifier = ClassifierMap[classifier_name][0]()

"""Fit the model"""

classifier.fit(X_train,y_train)

acc = classifier.score(X_test, y_test)    
print("Accuracy %.3f" %acc)

"""**Evaluation**"""

from sklearn.metrics import confusion_matrix, classification_report
y_pred = classifier.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print('\n')
print(classification_report(y_test, y_pred))

"""**Prediction**

Import the blindset
"""

# Importing the blindset

filename = '/content/drive/My Drive/ML/nodupblindtest.json'
bs = pd.read_json(filename, lines=True)
print('File loaded: %d samples.\n\n' %(len(db.index)))
print(bs)

"""Compute vectorizer terms again and do prediction over the new unlabeled set


"""

# In this way we are able to manipulate the file as a vector of numbers

i=13
item = np.array(list(bs.lista_asm))
new_x = vectorizer.transform(item)
new_y = classifier.predict(new_x)

#print('%s %s' %(item,new_y))

"""Show a random sample and print output onto a txt file"""

id = random.randrange(0,len(new_y))
print('%d %s %s' %(id,new_y[id],bs.lista_asm[id]))

f = open('1894954.txt',mode='w+')

for item in new_y:
  f.write(item + '\n')

f.close()